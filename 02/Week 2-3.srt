1
00:00:10,460 --> 00:00:14,620
In this plot, we will introduce the "Central Limit Theorem,"

2
00:00:14,620 --> 00:00:21,720
which is probably the single most important theorem in the theory of probability.

3
00:00:21,720 --> 00:00:24,000
The theorem helps to explain

4
00:00:24,000 --> 00:00:33,040
why the Gaussian or Normal distribution finds such widespread applications in natural and social sciences.

5
00:00:33,040 --> 00:00:36,540
Briefly stated, it says that

6
00:00:36,540 --> 00:00:40,180
the sum of N independent random variables,

7
00:00:40,180 --> 00:00:44,220
which can be drawn from any arbitrary,

8
00:00:44,240 --> 00:00:54,200
but well-behaved distributions will converge to a Gaussian distribution in the limit of large N.

9
00:00:54,200 --> 00:00:57,720
We will discuss this in more detail later,

10
00:00:57,720 --> 00:01:04,760
but first we will begin by working out some practical examples

11
00:01:04,760 --> 00:01:10,920
that show the central limit theorem in practice.

12
00:01:10,920 --> 00:01:14,040
In a previous lesson,

13
00:01:14,040 --> 00:01:19,900
we introduced the binomial distribution P(n,M),

14
00:01:19,900 --> 00:01:26,260
which describes a random process with two possible outcomes.

15
00:01:26,260 --> 00:01:30,020
The standard example is that of a coin toss,

16
00:01:30,020 --> 00:01:34,560
which can come up heads with probability p,

17
00:01:34,560 --> 00:01:39,920
and tails with probability (1-p).

18
00:01:39,920 --> 00:01:52,700
The probability to observe n heads after M throws is given by Eq. (C6).

19
00:01:52,700 --> 00:01:57,180
In the limit when both n and M are very large,

20
00:01:57,180 --> 00:02:04,020
we have seen that the binomial distribution converges to the Gaussian distribution

21
00:02:04,020 --> 00:02:21,040
with average μ_1=Mp and variance σ^2=Mp(1−p).

22
00:02:21,040 --> 00:02:23,300
We will see that

23
00:02:23,300 --> 00:02:29,780
this is a trivial application of the central limit theorem.

24
00:02:29,780 --> 00:02:35,480
It works because the result of each coin toss is given by

25
00:02:35,480 --> 00:02:43,420
an independent random process with a well-defined average and variance.

26
00:02:43,440 --> 00:02:52,300
While the proof for the equivalence has been given in the supplemental note,

27
00:02:52,300 --> 00:03:01,640
let us examine this by performing numerical experiments for various values of M using Python.

28
00:03:01,640 --> 00:03:03,200
As always,

29
00:03:03,220 --> 00:03:11,980
we begin by importing the necessary numerical and graphical libraries.

30
00:03:18,420 --> 00:03:20,540
In this code example,

31
00:03:20,540 --> 00:03:28,500
we assume we are using a fair coin, such that p = 0.5.

32
00:03:28,500 --> 00:03:32,900
The experiment consists in performing M coin tosses

33
00:03:32,900 --> 00:03:36,960
and counting the number of heads.

34
00:03:36,960 --> 00:03:38,840
To get reliable statistics,

35
00:03:38,840 --> 00:03:45,320
we repeat this for N = 100,000 times,

36
00:03:45,320 --> 00:03:50,500
and calculate the histogram of the data.

37
00:03:50,500 --> 00:03:56,800
The sampling is performed in one line using numpy's built-in binomial function,

38
00:03:56,840 --> 00:04:04,560
and the sampled data is stored as an array X of size N.

39
00:04:04,560 --> 00:04:13,520
Remember each element of the X array contains the number of heads after M coin tosses.

40
00:04:13,520 --> 00:04:20,820
We plot the histogram using the "hist" function.

41
00:04:20,820 --> 00:04:25,960
Finally, we compare the results of our experiment with

42
00:04:25,960 --> 00:04:31,880
the corresponding Gaussian distribution shown in Eq.(C1).

43
00:04:31,880 --> 00:04:36,940
For this, we generate an array of x values

44
00:04:36,940 --> 00:04:43,280
in a range 5 standard deviations to the right and left of the average value,

45
00:04:43,280 --> 00:04:49,640
and calculate the theoretical Gaussian distribution over this array.

46
00:04:49,640 --> 00:04:56,940
If you run this code example,

47
00:04:56,940 --> 00:05:03,740
the results are plotted and overlaid on the histogram.

48
00:05:03,740 --> 00:05:15,920
You should repeat this experiment for several values of M=1, 2, 4, 10, 50, 100 and 1000

49
00:05:15,920 --> 00:05:23,900
to see how the histogram converges towards the Gaussian distribution.

50
00:05:23,900 --> 00:05:30,160
The convergence is assumed in the limit of large n and M.

51
00:05:30,160 --> 00:05:33,540
In practice, you will see that

52
00:05:33,540 --> 00:05:43,940
a value of M = 50 already gives quite good agreement between the two distributions.

53
00:05:47,600 --> 00:05:52,680
Let us discuss what we have seen in the experiments.

54
00:05:52,680 --> 00:05:57,280
First we define a stochastic variable "s"

55
00:05:57,280 --> 00:06:01,580
which gives the result of a single binary choice

56
00:06:01,580 --> 00:06:06,780
that can be either 0 or 1.

57
00:06:06,780 --> 00:06:13,200
The total number of heads, after M tosses, is given by the sum of all "s",

58
00:06:13,220 --> 00:06:20,680
from the 1st to the M-th choice.

59
00:06:20,680 --> 00:06:26,840
This defines a new stochastic variable n of M.

60
00:06:26,840 --> 00:06:34,540
First, consider the case of a single coin toss for M=1,

61
00:06:34,540 --> 00:06:40,280
for which n of M is the same as a single binary choice s,

62
00:06:40,280 --> 00:06:45,100
as shown in Eq.(D1).

63
00:06:45,100 --> 00:06:50,840
Since this random variable is drawn from a binomial distribution,

64
00:06:50,840 --> 00:06:59,200
the average and variance are given by Eqs. (D2) and (D3).

65
00:06:59,240 --> 00:07:02,520
Namely, the average number of heads

66
00:07:02,520 --> 00:07:15,400
from one coin toss is equal to p, and the variance is equal to p*(1-p).

67
00:07:15,400 --> 00:07:20,600
Now, consider the case for arbitrary M.

68
00:07:20,660 --> 00:07:25,080
As we already discussed before,

69
00:07:25,080 --> 00:07:33,540
n of M is given by the sum all "s" up to the M-th choice,

70
00:07:33,540 --> 00:07:39,700
or equivalently, as the sum of M independent single coin toss results,

71
00:07:39,700 --> 00:07:46,720
n of M=1 as shown in Eq.(D4).

72
00:07:46,720 --> 00:07:49,920
In the limit when M is very large,

73
00:07:49,920 --> 00:07:55,100
we have already proven that this converges to a Gaussian.

74
00:07:55,100 --> 00:07:58,520
The average and variance of this distribution

75
00:07:58,540 --> 00:08:04,800
is just the average and variance of the distribution for a single coin toss

76
00:08:04,800 --> 00:08:13,840
multiplied by M as shown in Eqs. (D5) and (D6).

77
00:08:13,840 --> 00:08:20,100
The previous relation between binomial and Gaussian distributions is

78
00:08:20,160 --> 00:08:25,740
in fact an example of the Central Limit Theorem.

79
00:08:25,740 --> 00:08:34,020
It is valid not only for the sum of random variables drawn from binomial distributions,

80
00:08:34,080 --> 00:08:43,740
but it is applicable to any distributions with finite variance.

81
00:08:43,740 --> 00:08:49,340
Thus, assume we have a set of M independent random variables,

82
00:08:49,340 --> 00:08:53,380
which are drawn from the same distribution

83
00:08:53,380 --> 00:09:06,800
given by n of M=1 with average and variance μ_1 and σ^2 of M=1.

84
00:09:06,800 --> 00:09:11,920
The sum of these random variables is itself a random variable,

85
00:09:11,960 --> 00:09:15,560
and in the limit when M is very large,

86
00:09:15,560 --> 00:09:19,620
it converges to a Gaussian distribution.

87
00:09:19,620 --> 00:09:30,820
The average and variance of n of M >> 1 are given by Eqs. (D8) and (D9),

88
00:09:30,820 --> 00:09:38,260
which are indeed generalizations of Eqs. (D5) and (D6).

89
00:09:38,260 --> 00:09:42,460
The average and variance of this sum are then

90
00:09:42,460 --> 00:09:51,820
M times the average and variance of the individual distributions.

91
00:09:51,820 --> 00:09:57,540
One of the most useful forms of the Central Limit Theorem appears when

92
00:09:57,540 --> 00:10:04,220
considering the average of a series of stochastic variables, not just the sum.

93
00:10:04,220 --> 00:10:05,900
As before,

94
00:10:05,960 --> 00:10:12,860
let n of M=1 be a series of stochastic variables.

95
00:10:12,860 --> 00:10:15,100
we assume that

96
00:10:15,100 --> 00:10:24,780
they have average and variance of μ_1 and σ^2 of M=1.

97
00:10:24,780 --> 00:10:29,500
They should be independent and identically distributed,

98
00:10:29,520 --> 00:10:31,460
but other than that

99
00:10:31,460 --> 00:10:37,740
they can be drawn from any distribution with a finite variance.

100
00:10:37,740 --> 00:10:42,660
The average of these stochastic variables is given by

101
00:10:42,700 --> 00:10:48,040
the weighted sum in Eq. (D10).

102
00:10:48,040 --> 00:10:52,920
Apart from the factor of 1/M,

103
00:10:52,960 --> 00:10:58,280
this is exactly the same as we considered in the previous example.

104
00:10:58,280 --> 00:11:02,960
Now, we know that in the limit of very large M

105
00:11:02,960 --> 00:11:08,300
the distribution of this variable converges to a Gaussian.

106
00:11:08,300 --> 00:11:14,480
However, because of the 1/M factor in Eq. (D10)

107
00:11:14,480 --> 00:11:22,500
the average and variance are now given by Eqs. (D11) and (D12).

108
00:11:22,500 --> 00:11:24,140
Notice that

109
00:11:24,160 --> 00:11:35,840
the average value of n of M is now equal to the average value of n of M=1.

110
00:11:35,840 --> 00:11:37,760
More importantly,

111
00:11:37,780 --> 00:11:52,060
the variance of n of M is the variance of n of M=1 divided by M.

112
00:11:52,060 --> 00:11:59,820
Let us see another example of the Central Limit Theorem in practice.

113
00:11:59,820 --> 00:12:02,880
Now, instead of a binomial distribution,

114
00:12:02,900 --> 00:12:07,820
let us consider a continuous uniform distribution.

115
00:12:07,820 --> 00:12:13,480
We define a stochastic variable "x" of M=1,

116
00:12:13,480 --> 00:12:19,200
which is uniformly distributed between the unit interval.

117
00:12:19,200 --> 00:12:26,080
Thus, the probability density of observing any valuable x is constant,

118
00:12:26,080 --> 00:12:27,720
and equal to one

119
00:12:27,720 --> 00:12:31,740
if x lies in the unit interval,

120
00:12:31,740 --> 00:12:35,940
and zero if it is outside.

121
00:12:35,940 --> 00:12:40,220
The average and variance can be easily calculated analytically,

122
00:12:40,220 --> 00:12:49,780
and are given by 1/2 and 1/12, respectively.

123
00:12:49,800 --> 00:12:55,560
Now, define a new stochastic variable x of M,

124
00:12:55,560 --> 00:13:03,540
which is the sum of M of these uniform random variables.

125
00:13:03,540 --> 00:13:06,900
In the limit when M is very large,

126
00:13:06,900 --> 00:13:19,020
x of M is described by a Gaussian distribution with average and mean given by Eqs. (D8) and (D9).

127
00:13:19,060 --> 00:13:30,820
In this case, the average is just M/2 and the variance is M/12.

128
00:13:30,820 --> 00:13:36,060
In this numerical experiment, we will verify that

129
00:13:36,060 --> 00:13:41,920
the sum of uniformly distributed random numbers converges to a Gaussian distribution

130
00:13:41,920 --> 00:13:50,000
with average and variance defined in Eqs. (D17) and (D18).

131
00:13:50,000 --> 00:13:51,840
In the code example,

132
00:13:51,840 --> 00:13:59,200
we first set M, the number of random variables to add.

133
00:13:59,200 --> 00:14:02,220
You should try with several values of M,

134
00:14:02,220 --> 00:14:07,140
such as M=1, 2, 4, 10 and 100

135
00:14:07,140 --> 00:14:12,000
to see how the distribution converges to a Gaussian.

136
00:14:12,000 --> 00:14:18,560
Thus, one experimental run consists of generating M random variables,

137
00:14:18,660 --> 00:14:25,200
and adding them to generate a sample for the cumulative variable.

138
00:14:25,200 --> 00:14:31,580
We store these in the array X.

139
00:14:31,580 --> 00:14:33,780
To obtain reliable statistics,

140
00:14:33,780 --> 00:14:41,460
we repeat this experiment N = 100,000 times.

141
00:14:41,460 --> 00:14:45,000
Then, we plot the histogram of X

142
00:14:45,040 --> 00:14:50,260
together with the corresponding theoretical Gaussian distribution.

143
00:14:50,280 --> 00:14:55,780
The two should be equal in the limit 
when M is very large.

144
00:14:55,780 --> 00:15:00,180
In practice, how many uniform random variables

145
00:15:00,180 --> 00:15:09,700
must you add to obtain a Gaussian random variable?

146
00:15:09,700 --> 00:15:14,080
As you should see for yourselves,

147
00:15:14,080 --> 00:15:19,340
adding just 10 uniform random variables is already enough

148
00:15:19,340 --> 00:15:27,480
to reproduce a distribution which is very close to a Gaussian distribution.

